{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbc9df0",
   "metadata": {},
   "source": [
    "# Machine Learning Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0e042",
   "metadata": {},
   "source": [
    "Matheus Schmitz<br>\n",
    "<a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a><br>\n",
    "<a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747163fe",
   "metadata": {},
   "source": [
    "## Task 1: Object Oriented Programming for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0cfe7",
   "metadata": {},
   "source": [
    "Write a Python class, using principles of object-oriented design, to wrap a logistic regression estimator with the following functionality:\n",
    "\n",
    "\n",
    "**1. The ability to fit on training data:**  \n",
    ">self.fit(X, y)\n",
    "\n",
    "| I/O | Typing | Definiion |  \n",
    "|---|---|---|\n",
    "| Input | X : pd.DataFrame | Input features |\n",
    "| Input | y : np.ndarray | Ground truth labels as a numpy array of 0-s and 1-s. |\n",
    "| Output |  None | |\n",
    "\n",
    "**2. The ability to predict class labels on new data:**  \n",
    ">self.predict(X)\n",
    "\n",
    "| I/O | Typing | Definiion |  \n",
    "|---|---|---|\n",
    "| Input | X : pd.DataFrame | Input features |\n",
    "| Output |  np.ndarray | Ex: np.array([1, 0, 1]) |\n",
    "\n",
    "**3. The ability to predict the probability of each label:**  \n",
    ">self.predict_proba(X)\n",
    "\n",
    "| I/O | Typing | Definiion |  \n",
    "|---|---|---|\n",
    "| Input | X : pd.DataFrame | Input features |\n",
    "| Output |  np.ndarray | Ex: np.array([[0.2, 0.8], [0.9, 0.1], [0.5, 0.5]]) |\n",
    "\n",
    "**4. The ability to get the value of the following metrics: F1-score, LogLoss:**  \n",
    ">self.evaluate(X, y)\n",
    "\n",
    "| I/O | Typing | Definiion |  \n",
    "|---|---|---|\n",
    "| Input | X : pd.DataFrame | Input features |\n",
    "| Input | y : np.ndarray | Ground truth labels as a numpy array of 0-s and 1-s. |\n",
    "| Output | dict | Ex: {'f1_score': 0.3, 'logloss': 0.7} |\n",
    "\n",
    "**5. The ability to run K-fold cross validation to choose the best parameters:**  \n",
    "Note: Output the average scores across all CV validation partitions and best parameters  \n",
    ">self.tune_parameters(X, y)\n",
    "\n",
    "| I/O | Typing | Definiion |  \n",
    "|---|---|---|\n",
    "| Input | X : pd.DataFrame | Input features |\n",
    "| Input | y : np.ndarray | Ground truth labels as a numpy array of 0-s and 1-s. |\n",
    "| Output | dict | Ex: {'tol': 0.02, 'fit_intercept': False, 'solver': 'sag', 'scores': {'f1_score': 0.3, 'logloss': 0.7}} |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b5948",
   "metadata": {},
   "source": [
    "## Task 2: Unit Testing\n",
    "\n",
    "Please write the unit tests to check whether your model:  \n",
    "- is reproducible  \n",
    "- can handle missing values  \n",
    "- can handle new category levels at prediction time  \n",
    "- returns results in the expected format  \n",
    "- other useful unit tests you may think of (if time allows)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573eda63",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To test your implementation, please use the attached dataset ``sample_dataset.csv``. It contains a binary classification target for predicting loan defaults, where the column ``is_bad`` contains the target variable and all other columns contain features. Make sure to explore the dataset to think of feature corner cases which your model must handle.\n",
    "\n",
    "Assume the data can have numeric and categorical variables. Both your training and prediction functions will take in a two-dimensional pandas DataFrame with a mixture of categorical and numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e3d4f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e999e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 25\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6686a",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20064380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_dataset.csv')\n",
    "X = df.drop(columns='is_bad')\n",
    "y = np.array(df['is_bad'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a75f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>is_bad</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>purpose_cat</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>policy_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>not verified</td>\n",
       "      <td>n</td>\n",
       "      <td>medical</td>\n",
       "      <td>766xx</td>\n",
       "      <td>TX</td>\n",
       "      <td>10.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12087</td>\n",
       "      <td>12.1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>39216.0</td>\n",
       "      <td>not verified</td>\n",
       "      <td>n</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>660xx</td>\n",
       "      <td>KS</td>\n",
       "      <td>9.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10114</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>RENT</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>not verified</td>\n",
       "      <td>n</td>\n",
       "      <td>credit card</td>\n",
       "      <td>916xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>11.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PC4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  is_bad emp_length home_ownership  annual_inc verification_status  \\\n",
       "0   1       0         10       MORTGAGE     50000.0        not verified   \n",
       "1   2       0          1           RENT     39216.0        not verified   \n",
       "2   3       0          4           RENT     65000.0        not verified   \n",
       "\n",
       "  pymnt_plan         purpose_cat zip_code addr_state  debt_to_income  \\\n",
       "0          n             medical    766xx         TX           10.87   \n",
       "1          n  debt consolidation    660xx         KS            9.15   \n",
       "2          n         credit card    916xx         CA           11.24   \n",
       "\n",
       "   delinq_2yrs  inq_last_6mths  mths_since_last_delinq  \\\n",
       "0          0.0             0.0                     NaN   \n",
       "1          0.0             2.0                     NaN   \n",
       "2          0.0             0.0                     NaN   \n",
       "\n",
       "   mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  \\\n",
       "0                     NaN      15.0      0.0      12087        12.1   \n",
       "1                     NaN       4.0      0.0      10114        64.0   \n",
       "2                     NaN       4.0      0.0         81         0.6   \n",
       "\n",
       "   total_acc initial_list_status  collections_12_mths_ex_med  \\\n",
       "0       44.0                   f                         0.0   \n",
       "1        5.0                   f                         0.0   \n",
       "2        8.0                   f                         0.0   \n",
       "\n",
       "   mths_since_last_major_derog policy_code  \n",
       "0                            1         PC4  \n",
       "1                            2         PC1  \n",
       "2                            3         PC4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44d8891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Id                           10000 non-null  int64  \n",
      " 1   is_bad                       10000 non-null  int64  \n",
      " 2   emp_length                   10000 non-null  object \n",
      " 3   home_ownership               10000 non-null  object \n",
      " 4   annual_inc                   9999 non-null   float64\n",
      " 5   verification_status          10000 non-null  object \n",
      " 6   pymnt_plan                   10000 non-null  object \n",
      " 7   purpose_cat                  10000 non-null  object \n",
      " 8   zip_code                     10000 non-null  object \n",
      " 9   addr_state                   10000 non-null  object \n",
      " 10  debt_to_income               10000 non-null  float64\n",
      " 11  delinq_2yrs                  9995 non-null   float64\n",
      " 12  inq_last_6mths               9995 non-null   float64\n",
      " 13  mths_since_last_delinq       3684 non-null   float64\n",
      " 14  mths_since_last_record       840 non-null    float64\n",
      " 15  open_acc                     9995 non-null   float64\n",
      " 16  pub_rec                      9995 non-null   float64\n",
      " 17  revol_bal                    10000 non-null  int64  \n",
      " 18  revol_util                   9974 non-null   float64\n",
      " 19  total_acc                    9995 non-null   float64\n",
      " 20  initial_list_status          10000 non-null  object \n",
      " 21  collections_12_mths_ex_med   9968 non-null   float64\n",
      " 22  mths_since_last_major_derog  10000 non-null  int64  \n",
      " 23  policy_code                  10000 non-null  object \n",
      "dtypes: float64(11), int64(4), object(9)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3116fa1d",
   "metadata": {},
   "source": [
    "### Design OOP ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbf28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OOPModel():\n",
    "    \n",
    "    '''Wrapper class for Logistic Regression with built-in preprocessing'''\n",
    "\n",
    "    def __init__(self, nan_token: str = 'na'):\n",
    "        super(OOPModel, self).__init__()\n",
    "        self.pipeline = Pipeline([(\"scaler\", MinMaxScaler()),\n",
    "                                  (\"model\", LogisticRegression())])\n",
    "        self.__trained = False\n",
    "        self.nan_token = nan_token\n",
    "\n",
    "        # Logistic Regression parameters\n",
    "        self.pipeline['model'].penalty = 'l2'\n",
    "        self.pipeline['model'].tol = 10e-5\n",
    "        self.pipeline['model'].C = 10e-1\n",
    "        self.pipeline['model'].class_weight = 'balanced'\n",
    "        self.pipeline['model'].solver = 'saga'\n",
    "        self.pipeline['model'].max_iter = 10e3\n",
    "        self.pipeline['model'].n_jobs = -1\n",
    "\n",
    "    def _preprocess_X_train(self, X: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "        self.preprocessing = {} # Store rules on how to preprocess data at inference time\n",
    "        unique_cols = X.columns[X.nunique() == X.shape[0]] # Get columns with unique identifiers and remove them\n",
    "        invariant_cols = X.columns[X.nunique() == 1] # Get columns without feature variance and remove them\n",
    "        removal_cols = np.append(unique_cols, invariant_cols)\n",
    "        X = X.drop(columns = removal_cols)\n",
    "        for col in removal_cols: \n",
    "            self.preprocessing[col] = 'delete'\n",
    "        # Rule for converting strings to missing values\n",
    "        X = X.replace(self.nan_token, np.NaN)\n",
    "        for column in X.columns:\n",
    "            # Convert any numeric columns loaded as strings to float and fill missing values with median\n",
    "            try:\n",
    "                X[column] = X[column].astype(float)\n",
    "                X[column] = X[column].fillna(X[column].median())\n",
    "                self.preprocessing[column] = 'numeric'\n",
    "            # Else column is categorical, one-hot encode it\n",
    "            except ValueError:\n",
    "                x = pd.get_dummies(X[column])\n",
    "                X = X.drop(columns=column)\n",
    "                # If the ratio of unique categories to samples is too large, drop the feature\n",
    "                if x.shape[1] > np.sqrt(X.shape[0]):\n",
    "                    self.preprocessing[column] = 'delete' \n",
    "                else:                \n",
    "                    self.preprocessing[column] = x.columns.tolist()\n",
    "                    x.columns = [str(column)+'_'+str(col) for col in x.columns] # rename the dummy columns prepending the variable name\n",
    "                    X = pd.concat([X, x], axis='columns') \n",
    "        return X.sort_index(axis='columns')\n",
    "\n",
    "    def _preprocess_X_test(self, X: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "        # Rule for converting strings to missing values\n",
    "        X = X.replace(self.nan_token, np.NaN)\n",
    "        # Preprocess each column according to the rules defined during training\n",
    "        for column in X.columns:\n",
    "            # Delete columns identified as unique during training\n",
    "            if self.preprocessing[column] == 'delete':\n",
    "                X = X.drop(columns=column)\n",
    "            # Float columns become float, and anything that cannot be converted is replaced by median\n",
    "            elif self.preprocessing[column] == 'numeric':\n",
    "                X[column] = pd.to_numeric(X[column], errors='coerce') # coerce errors to NaN\n",
    "                X[column] = X[column].fillna(X[column].median())\n",
    "            # Categorial columns get one-hot encoded\n",
    "            elif type(self.preprocessing[column]) == list:\n",
    "                x = pd.get_dummies(X[column])                    \n",
    "                x = x.drop(columns=[col for col in x if col not in self.preprocessing[column]]) # Drop any category level not seen in training\n",
    "                x[[col for col in self.preprocessing[column] if col not in x]] = 0 # Ensure all training category levels are present in the test data\n",
    "                x.columns = [str(column)+'_'+str(col) for col in x.columns] # rename the dummy columns prepending the variable name\n",
    "                X = X.drop(columns=column)\n",
    "                X = pd.concat([X, x], axis='columns')\n",
    "            else:\n",
    "                raise ValueError(f\"Received an unexpected new column '{column}' which could not be properly handled\")\n",
    "        return X.sort_index(axis='columns')\n",
    "\n",
    "    def _check_trained(self) -> None:\n",
    "        if self.__trained == False:\n",
    "            raise Exception(\"Model not yet trained. Call 'fit' with appropriate arguments before using this estimator.\")\n",
    "\n",
    "    def _check_inputs(self, X: pd.core.frame.DataFrame, y: np.ndarray = None) -> None:\n",
    "        if not type(X) == pd.core.frame.DataFrame: raise TypeError(\"X should be a pandas.core.frame.DataFrame\")\n",
    "        if self.__trained == True and not len(X.columns) == len(self.__features): raise ValueError(\"X should have the same columns as the training data passed to 'fit'\")\n",
    "        if self.__trained == True and not all(X.columns == self.__features): raise ValueError(\"X should have the same columns as the training data passed to 'fit'\")\n",
    "        if y is not None:\n",
    "            if not type(y) == np.ndarray: raise TypeError(\"y should be a numpy.ndarray\")\n",
    "            if not np.unique(y).tolist() == [0, 1]: raise ValueError(\"y must contain negative (0) and positive (1) labels, and nothing else\")\n",
    "            if not X.shape[0] == y.shape[0]: raise ValueError(f\"X and y must have the same number of samples, got {X.shape[0]} X samples and {y.shape[0]} y samples\")\n",
    "\n",
    "    def fit(self, X: pd.core.frame.DataFrame, y: np.ndarray) -> None:\n",
    "        self._check_inputs(X, y)\n",
    "        self.__features = X.columns\n",
    "        X = self._preprocess_X_train(X)\n",
    "        self.pipeline.fit(X, y)\n",
    "        self.__trained = True\n",
    "\n",
    "    def predict(self,  X: pd.core.frame.DataFrame) -> np.ndarray:\n",
    "        self._check_trained()\n",
    "        self._check_inputs(X)\n",
    "        X = self._preprocess_X_test(X)\n",
    "        predictions = self.pipeline.predict(X)  \n",
    "        return predictions\n",
    "\n",
    "    def predict_proba(self,  X: pd.core.frame.DataFrame) -> np.ndarray:\n",
    "        self._check_trained()\n",
    "        self._check_inputs(X)\n",
    "        X = self._preprocess_X_test(X)\n",
    "        predictions = self.pipeline.predict_proba(X)  \n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, X: pd.core.frame.DataFrame, y: np.ndarray) -> dict:\n",
    "        self._check_trained()\n",
    "        self._check_inputs(X, y)\n",
    "        X = self._preprocess_X_test(X)\n",
    "        f1score = f1_score(y, self.pipeline.predict(X))\n",
    "        logloss = log_loss(y, self.pipeline.predict_proba(X))\n",
    "        return {'f1_score': f1score, 'log_loss': logloss}\n",
    "\n",
    "    def tune_parameters(self, X: pd.core.frame.DataFrame, y: np.ndarray) -> dict:\n",
    "        self._check_inputs(X, y)\n",
    "        self.__features = X.columns\n",
    "        X = self._preprocess_X_train(X)\n",
    "        y_labels = sorted(np.unique(y))\n",
    "        param_grid = {'model__penalty': ['l1', 'l2'],\n",
    "                      'model__tol': np.logspace(-6, -4, 3),\n",
    "                      'model__C': np.logspace(-2, 2, 5)}\n",
    "        self.model_gridCV = GridSearchCV(self.pipeline, param_grid, cv=5, n_jobs=-1, scoring=['f1', 'neg_log_loss'], refit='neg_log_loss', verbose=3)\n",
    "        self.model_gridCV.fit(X, y)\n",
    "        self.pipeline = self.model_gridCV.best_estimator_ # Updates the existing model with the optimal model found\n",
    "        self.__trained = True\n",
    "        best_mean_f1 = self.model_gridCV.cv_results_['mean_test_f1'][self.model_gridCV.best_index_]\n",
    "        best_mean_log_loss = self.model_gridCV.cv_results_['mean_test_neg_log_loss'][self.model_gridCV.best_index_] * -1 # invert signal from negative log_loss\n",
    "        return {'penalty': self.model_gridCV.best_params_['model__penalty'],\n",
    "                'tol': self.model_gridCV.best_params_['model__tol'],\n",
    "                'C': self.model_gridCV.best_params_['model__C'],\n",
    "                'scores': {'f1_score': best_mean_f1, 'log_loss': best_mean_log_loss}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533c601",
   "metadata": {},
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82a166f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d5cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAssignment(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        pass\n",
    "\n",
    "    def setUp(self):\n",
    "        self.model = OOPModel()\n",
    "        self.df = pd.read_csv('sample_dataset.csv')\n",
    "        self.X = self.df.drop(columns='is_bad')\n",
    "        self.y = np.array(self.df['is_bad'])\n",
    "\n",
    "    def tearDown(self):\n",
    "        del self.model, self.df, self.X, self.y\n",
    "   \n",
    "    def test_missing_values_fit(self):\n",
    "        for row, column in enumerate(self.X.columns):\n",
    "            self.X.at[row, column] = None\n",
    "        self.model.fit(self.X, self.y)\n",
    "\n",
    "    def test_missing_values_predict(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        for row, column in enumerate(self.X.columns):\n",
    "            self.X.at[row, column] = None\n",
    "        predictions = self.model.predict(self.X)\n",
    "\n",
    "    def test_new_category_levels_at_prediction_time(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        self.X.at[0, 'policy_code'] = 'new_category_level'\n",
    "        self.X.at[1, 'home_ownership'] = 'another_category_level'\n",
    "        predictions = self.model.predict(self.X)\n",
    "\n",
    "    def test_result_format_fit(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        predictions = self.model.predict(self.X)\n",
    "        self.assertIsInstance(predictions, np.ndarray, msg='Output shoud be of numpy.ndarray type')\n",
    "\n",
    "    def test_result_format_evaluate(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        output = self.model.evaluate(self.X, self.y)\n",
    "        self.assertIsInstance(output, dict, msg='tune_parameters() should output a dictionary')\n",
    "        self.assertEqual(['f1_score', 'log_loss'], list(output.keys()), msg=\"Output should contain a key 'scores' with f1_score and log_loss\")\n",
    "\n",
    "    def test_result_format_tune_parameters(self):\n",
    "        output = self.model.tune_parameters(self.X, self.y)\n",
    "        self.assertIsInstance(output, dict, msg='tune_parameters() should output a dictionary')\n",
    "        self.assertEqual(['f1_score', 'log_loss'], list(output['scores'].keys()), msg=\"Output should contain a key 'scores' with f1_score and log_loss\")\n",
    "\n",
    "    def test_probability_range(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        probabilities = self.model.predict_proba(self.X)\n",
    "        self.assertTrue(0 <= probabilities.min(), msg='Lowest probability should be >= 0')\n",
    "        self.assertTrue(probabilities.max() <= 1, msg='Highest probability should be <= 1')\n",
    "\n",
    "    def test_new_column_error(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        self.X['new_column'] = 'new_value'\n",
    "        with self.assertRaises(ValueError) as cm:\n",
    "            predictions = self.model.predict(self.X)\n",
    "\n",
    "    def test_missing_column_error(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        self.X = self.X.drop(columns=['zip_code'])\n",
    "        with self.assertRaises(ValueError) as cm:\n",
    "            predictions = self.model.predict(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c3bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 755.876s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1c5fd078a88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run unit test\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bf027",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "Matheus Schmitz<br>\n",
    "<a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a><br>\n",
    "<a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
